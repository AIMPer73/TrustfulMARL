# TrustfulMARL


[1] F. A. Oliehoek, C. Amato. A concise introduction to decentralizedPOMDPs, Berlin: Springer, 2016.

[2] P. Doshi, Y. Zeng, Q. Chen. Graphical models for interactive POMDPs:representations and solutions, Autonomous Agents and Multi-AgentSystems, 2009, 18(3):376-386.

[3] L. S. Shapley, Stochastic Games, in: Proceeding of the NationalAcademy of Sciences , 1953, 39(10):1095-1100.

[4] M. Littman, Markov games as a framework for multi-agent reinforcementlearning, in: Proceeding of the Eleventh International Conferenceon International Conference on Machine Learning. 1994: 157-163.

[5] V. Kovaik, M. Schmid, N. Burch, et al. Rethinking formal models ofpartially observable multiagent decision making , Arti_cial Intelligence,2022, 303: 623-645.

[6] E. Lockhart, M. Lanctot, J. Perolat, et al. Computing approximate equilibriain sequential adversarial games by exploitability descent, 2019,arXiv preprint arXiv:1903.05614.

[7] Y. Shoham, K. Leyton-Brown. Multiagent systems algorithmic, gametheoretic,and logical foundations, New York:Cambridge UniversityPress, 2009.

[8] Q. Cui, L. F. Yang. Minimax sample complexity for turn-based stochasticgame, 2020, arXiv preprint arXiv:2011.14267.

[9] T. Roughgarden. Twenty lectures on algorithmic game theory, NewYork: Cambridge University Press, 2016.

[10] A. Blum, N. Haghtalab, M. T. Hajiaghayi, et al. Computing Stackelbergequilibria of large general-sum games, in: Proceeding of the InternationalSymposium on Algorithmic Game Theory, 2019, pp. 168{182.

[11] D. Milec, J. Cerny, V. Lisy, et al. Complexity and algorithms for exploitingquantal opponents in large two-player games, in: Proceeding ofthe AAAI Conference on Arti_cial Intelligence. 2021, 35(6): 5575-5583.

[12] D. Balduzzi, K. Tuyls, J. Perolat, et al, Re-evaluating evaluation, in:Proceeding of the 32nd International Conference on Neural InformationProcessing Systems, 2018, 3272{3283.

[13] S. H. Li, Y. Wu, X. Y. Cui, et al. Robust multi-agent reinforcementlearning via minimax deep deterministic policy gradient, in: Proceedingof the AAAI Conference on Arti_cial Intelligence, 2019, 33:4213-4220.

[14] Y. Yabu, M. Yokoo, A. Iwasaki. Multiagent planning with tremblinghandperfect equilibrium in multiagent POMDPs, in: Proceeding of thePaci_c Rim International Conference on Multi-Agents, 2017, 13-24.

[15] A. Ghoroghi, Multi-games and Bayesian Nash equilibriums, London:University of London, 2015.

[16] X. Xu, Q. Zhao. Distributed no-regret learning in multi-agent, Systems.IEEE Signal Processing Magazine, 2020, 37(3):84-91.

[17] Y. Sun, X. Wei, Z. Yao , et al. Analysis of network attack and defensestrategies based on pareto optimum, Electronics, 2018, 7(3):36 .

[18] X. T. Deng, N. Y. Li, D. Mguni, et al. On the complexity of computingMarkov perfect equilibrium in general-sum stochastic games, 2021, arXivpreprint arXiv:2109.01795.

[19] B. Nicola, C. Basilico, D. N. Andrea, et al. Computing the team{maxminequilibrium in single{team single{adversary team games, IntelligenzaArti_ciale, 2017, 11(1):67-79.

[20] A. Celli, N. Gatti. Computational results for extensive-form adversarialteam games, 2017 arXiv preprint arXiv:1711.06930.

[21] Y. Z. Zhang, B. An. Computing team-maxmin equilibria in zero-summultiplayer extensive-form games, in: Proceeding of the AAAI Conferenceon Arti_cial Intelligence, 2020, 34(2):2318-2325.

[22] S. X. Li, Y. Z. Zhang, X. R. Wang, et al. CFR-MIX: Solving imperfectinformation extensive-form games with combinatorial action space, 2021, arXiv preprint arXiv:2105.08440.

[23] G. Probo, Multi-team games in adversarial settings: ex-ante coordinationand independent team members algorithms, Milano: Politecnico DiMilano, 2019.

[24] L. E. Ortiz, R. E. Schapire, S. M. Kakade. Maximum entropy correlatedequilibria, in: Proceeding of the Eleventh International Conference onArti_cial Intelligence and Statistics, PMLR, 2007: 347-354.

[25] I. Gemp, R. Savani, M. Lanctot, et al. Sample-based approximationof Nash in large many-player games via gradient descent, 2021, arXivpreprint arXiv:2106.01285.

[26] G. Farina, T. Bianchi, T. Sandholm. Coarse correlation in extensiveformgames, in: Proceeding of the AAAI Conference on Arti_cial Intelligence.2020, 34(02): 1934-1941.

[27] G. Farina, A. Celli, A. Marchesi, et al. Simple uncoupled no-regretlearning dynamics for extensive-form correlated equilibrium, 2021, arXivpreprint arXiv:2104.01520.

[28] Q. M. Xie, Y. D. Chen, Z. R. Wang, et al. Learning zero-sumsimultaneous-move Markov games using function approximation andcorrelated equilibrium, 2020, arXiv preprint arXiv:2002.07066.

[29] S. J. Huang, P. Yi. Distributed best response dynamics for Nash equilibriumseeking in potential games, Control Theory and Technology, 2020,18(3): 324-332.

[30] B. Bosansky, C. Kiekintveld, V. Lisy, et al. An exact double-oracle algorithmfor zero-sum extensive-form games with imperfect information,Journal of Arti_cial Intelligence Research, 2014, 51 :829-866.

[31] S. Sonzogni, Depth-limited approaches in adversarial team games, Milano:Politecnico Di Milano, 2019.

[32] Y. Z. Zhang, B. An. Converging to team maxmin equilibria in zero-summultiplayer games, in: Proceeding of the International Conference onMachine Learning. PMLR, 2020: 11033-11043.

[33] M. Tan. Multi-agent reinforcement learning: Independent vs. cooperativeagents, in: Proceeding of the Tenth international conference onmachine learning, 1993, 330{337.

[34] M. Laetitia, L. Guillaume, L. F. Nadine, Hysteretic Q Learning: analgorithm for decentralized reinforcement learning in cooperative multiagentteams, in: Proceeding of the IEEE/RSJ International Conferenceon Intelligent Robots and Systems, 2007: 64{69.

[35] L. Matignon, G. Laurent, N. L. Fort-Piat. A study of FMQ heuristic incooperative multi-agent games, in: Proceeding of the 7th InternationalConference on Autonomous Agents and Multiagent Systems. Workshop10: Multi-Agent Sequential Decision Making in Uncertain Multi-AgentDomains, AAMAS, 2008, 1: 77-91.

[36] E.Wei, S. Luke, Lenient learning in independent-learner stochastic cooperativegames

[J], Journal Machine Learning Research 2016, 17(1): 2914-2955.

[37] G. Palmer. Independent learning approaches: overcoming multi-agentlearning pathologies in team-games, Liverpool: University of Liverpool,2020.

[38] S. Sukhbaatar, R. Fergus. Learning multiagent communication withbackpropagation, in: Proceeding of the 30th International Conferenceon Neural Information Processing Systems, 2016, 29: 2244-2252.

[39] P. Peng, Y. Wen, Y. D. Yang, et al. Multiagent bidirectionallycoordinatednets: emergence of human-level coordination in learning toplay StarCraft combat games, 2017, arXiv preprint arXiv:1703.10069.

[40] N. F. Jakob, F. Gregory, A. Triantafyllos, et al, Counterfactual multiagentpolicy gradients, in: Proceeding of the AAAI conference on arti-_cial intelligence. 2018, 32(1): 2974-2982.

[41] R. Lowe, Y. Wu, A. Tamar, et al Multi-agent actor-critic for mixedcooperative-competitive environments, in: Proceeding of the 31st InternationalConference on Neural Information Processing Systems. 2017:6382-6393.

[42] E. Wei, D. Wicke, D. Freelan, et al, Multiagent soft q-learning, 2018,arXiv preprint arXiv:1804.09817.

[43] P. Sunehag, G. Lever, A. Gruslys, et al., Value-decomposition networksfor cooperative multi-agent learning based on team reward, in: Proceedingof the 17th International Conference on Autonomous Agents andMultiAgent Systems, International Foundation for Autonomous Agentsand Multiagent Systems, 2018: 2085{2087.

[44] T. Rashid, M. Samvelyan, C. S. Witt, et al. Qmix: Monotonic valuefunction factorisation for deep multi-agent reinforcement learning, in:Proceeding of the International Conference on Machine Learning, 2018:4292{4301.

[45] A. Mahajan, T. Rashid, M. Samvelyan, et al. MAVEN: Multi-AgentVariational Exploration, 2019, arXiv preprint arXiv:1910.07483.

[46] K. Son, D. Kim, W. J. Kang, et al. Qtran: Learning to factorize withtransformation for cooperative multi-agent reinforcement learning, in:Proceeding of the International Conference on Machine Learning, 2019:5887{5896.

[47] Y. D. Yang, Y. Wen, L. H. Chen, et al. Multi-agent determinantal qlearning,2020, arXiv preprint arXiv:2006.01482.

[48] C. Yu, A. Velu, E. Vinitsky, et al. The Surprising e_ectiveness ofMAPPO in cooperative, multi-agent games, 2021, arXiv preprintarXiv:2103.01955.

[49] J. H. Wang, Y. Zhang, T. K. Kim, et al. Shapley q-value: a local rewardapproach to solve global reward games, in: Proceeding of the AAAIConference on Arti_cial Intelligence, 2020, 34(5):7285-7292.

[50] M. Riedmiller. Neural _tted Q iteration{_rst experiences with a datae_cient neural reinforcement learning method, in: Proceeding of theEuropean Conference on Machine Learning, 2005: 317-328.

[51] A. Nedic, A. Olshevsky, W. Shi. Achieving geometric convergence fordistributed optimization over time-varying graphs, SIAM Journal onOptimization, 2017, 27(4): 2597{2633.

[52] K. Q. Zhang, Z. R. Yang, H. Liu, et al. Fully decentralized multi-agentreinforcement learning with networked agents, 2018, arXiv preprintarXiv:1802.08757.

[53] G. N. Qu, Y. H. Lin, A. Wierman, et al. Scalable multi-agent reinforcementlearning for networked systems with average reward, 2020, arXivpreprint arXiv:2006.06626.

[54] T. Chu, S. Chinchali, S. Katti, Multi-agent reinforcement learning fornetworked system control , 2020, arXiv preprint arXiv:2004.01339.

[55] A. Lesage-Landry, D. S. Callaway. Approximate multi-agent _tted qiteration, 2021, arXiv preprint arXiv:2104.09343.

[56] K. R. Varshney, Trustworthy machine learning, Chappaqua, NY, 2021.

[57] M. Xu, Z. Liu, P. Huang, et al., Trustworthy Reinforcement LearningAgainst Intrinsic Vulnerabilities: Robustness, Safety, and Generalizability,2022, arXiv preprint arXiv:2209.08025.

[58] K. Tuyls, Multiagent Learning: From Fundamentals to FoundationModels, in: Proc. of the 2023 Int. Conf. on Autonomous Agents andMultiagent Systems, 2023.

[59] K. Tuyls and G. Weiss, Multiagent learning: Basics, challenges, andprospects, Ai Magazine, vol. 33, no. 3, p. 41, 2012.

[60] M. Bowling and M. Veloso, Multiagent learning using a variable learningrate, Arti_cial Intelligence, vol. 136, no. 2, pp. 215-250, 2002.

[61] J. Hu and M. P. Wellman, Nash Q-learning for general-sum stochasticgames, Journal of Machine Learning Research, vol. 4, pp. 1039-1069,Nov. 2003.

[62] M. L. Littman, Value-function reinforcement learning in Markov games,Cognitive Systems Research, vol. 2, no. 1, pp. 55-66, 2001.

[63] P. Sunehag, G. Lever, A. Gruslys, et al., Value-Decomposition Networksfor Cooperative Multi-Agent Learning Based on Team Reward, in: Proc.of the 17th Int. Conf. on Autonomous Agents and MultiAgent Systems,2018, pp. 2085-2087.

[64] T. Rashid, M. Samvelyan, C. Schroeder, et al., Qmix: Monotonic valuefunction factorisation for deep multi-agent reinforcement learning, in:Int. Conf. on Machine Learning (ICML), PMLR, 2018, pp. 4295-4304.

[65] J. Foerster, G. Farquhar, T. Afouras, et al., Counterfactual multi-agentpolicy gradients, in: Proc. of the AAAI Conf. on Arti_cial Intelligence,vol. 32, no. 1, 2018.

[66] R. Lowe, Y. I. Wu, A. Tamar, et al., Multi-agent actor-critic for mixedcooperative-competitive environments, in: Advances in Neural InformationProcessing Systems (NeurIPS), vol. 30, 2017.

[67] C. Yu, A. Velu, E. Vinitsky, et al., The Surprising e_ectivenessof MAPPO in cooperative, multi-agent games, arXiv preprintarXiv:2103.01955, 2021.

[68] J. G. Kuba, R. Chen, M. Wen, et al., Trust region policy optimisationin multi-agent reinforcement learning, arXiv preprint arXiv:2109.11251,2021.

[69] D. Balduzzi, K. Tuyls, J. Perolat, et al., Re-evaluating evaluation, in:Proc. of the 32nd Int. Conf. on Neural Information Processing Systems(NeurIPS), 2018, pp. 3272-3283.

[70] S. Omidsha_ei, C. Papadimitriou, G. Piliouras, et al., _-rank: Multiagentevaluation by evolution, Scienti_c Reports, vol. 9, no. 1, article9937, 2019.

[71] O. Vinyals, I. Babuschkin, W. M. Czarnecki, et al., Grandmaster levelin StarCraft II using multi-agent reinforcement learning, Nature, vol.575, no. 7782, pp. 350-354, 2019.

[72] M. Lanctot, V. Zambaldi, A. Gruslys, et al., A uni_ed game-theoreticapproach to multiagent reinforcement learning, in: Proc. of the 31st Int.Conf. on Neural Information Processing Systems (NeurIPS), 2017, pp.4193-4206.

[73] J. Devlin, M. W. Chang, K. Lee, et al., Bert: Pre-training of deepbidirectional transformers for language understanding, arXiv preprintarXiv:1810.04805, 2018.

[74] Team A. A., J. Bauer, K. Baumli, et al., Human-timescale adaptationin an open-ended task space, 2023, arXiv preprint arXiv:2301.07608.

[75] J. Abramson, A. Ahuja, F. Carnevale, et al., Improving multimodalinteractive agents with reinforcement learning from human feedback,2022, arXiv preprint arXiv:2211.11602.

[76] C. Gulcehre, Z. Wang, A. Novikov, et al., RL Unplugged: A suite ofbenchmarks for o_ine reinforcement learning, Advances in Neural InformationProcessing Systems (NeurIPS), vol. 33, pp. 7248-7259, 2020.

[77] Z. Zhu, H. Zhao, H. He, et al., Di_usion Models for ReinforcementLearning: A Survey, 2023, arXiv preprint arXiv:2311.01223.

[78] A. Oroojlooy, D. Haezhad, A review of cooperative multi-agent deepreinforcement learning, Applied Intelligence, vol. 53, no. 11, pp. 13677-13722, 2023.

[79] Z. Zhou, G. Liu, Y. Tang, Multi-Agent Reinforcement Learning: Methods,Applications, Visionary Prospects, and Challenges, arXiv preprintarXiv:2305.10091, 2023.

[80] Y. Du, J. Z. Leibo, U. Islam, et al., A Review of Cooperation in MultiagentLearning, 2023, arXiv preprint arXiv:2312.05162.

[81] L. Yuan, Z. Zhang, L. Li, et al., A Survey of Progress on CooperativeMulti-agent Reinforcement Learning in Open Environment, 2023., arXivpreprint arXiv:2312.01058

[82] D. Kudithipudi, M. Aguilar-Simon, J. Babb, et al., Biological underpinningsfor lifelong learning machines, Nature Machine Intelligence, vol.4, no. 3, pp. 196{210, 2022.

[83] K. Khetarpal, M. Riemer, I. Rish, et al., Towards continual reinforcementlearning: A review and perspectives, Journal of Arti_cial IntelligenceResearch, vol. 75, pp. 1401{1476, 2022.

[84] G. I. Parisi, R. Kemker, J. L. Part, et al., Continual lifelong learningwith neural networks: A review, Neural Networks, vol. 113, pp. 54{71,2019.

[85] J. Kirkpatrick, R. Pascanu, N. Rabinowitz, et al., Overcoming catastrophicforgetting in neural networks, Proceedings of the NationalAcademy of Sciences, vol. 114, no. 13, pp. 3521{3526, 2017.

[86] D. Rolnick, A. Ahuja, J. Schwarz, et al., Experience replay for continuallearning, in: Advances in Neural Information Processing Systems(NeurIPS), pp. 348{358, 2018.

[87] Y. Huang, K. Xie, H. Bharadhwaj, et al., Continual model-based reinforcementlearning with hypernetworks, in: Proceedings of the IEEEInternational Conference on Robotics and Automation, pp. 799{805,2021.

[88] Z. Wang, C. Chen, D. Dong, et al., Lifelong incremental reinforcementlearning with online Bayesian inference, IEEE Transactions on NeuralNetworks and Learning Systems, vol. 33, no. 8, pp. 4003{4016, 2022.

[89] S. Kessler, J. Parker-Holder, P. J. Ball, et al., Same state, di_erent task:Continual reinforcement learning without interference, in: Proceedingsof the AAAI Conference on Arti_cial Intelligence, pp. 7143{7151, 2022.

[90] J. B. Gaya, T. Doan, L. Caccia, et al., Building a subspace of policiesfor scalable continual learning, in: International Conference on LearningRepresentations (ICLR), 2023.

[91] A. Mallya, S. Lazebnik, Packnet: Adding multiple tasks to a singlenetwork by iterative pruning, in: Proceedings of the IEEE Conferenceon Computer Vision and Pattern Recognition (CVPR), pp. 7765{7773,2018.

[92] H. Nekoei, A. Badrinaaraayanan, A. Courville, et al., Continuous coordinationas a realistic scenario for lifelong learning, in: Proceedings of theInternational Conference on Machine Learning (ICML), pp. 8016{8024,2021.

[93] L. Yuan, L. Li, Z. Zhang, et al., Multi-agent continual coordinationvia progressive task contextualization, 2023, arXiv preprintarXiv:2305.13937.

[94] L. Yuan, L. Li, Z. Zhang, et al., Learning to coordinate with anyone,2023, arXiv preprint arXiv:2309.12633.

[95] F. M. Luo, T. Xu, H. Lai, et al., A survey on model-based reinforcementlearning, 2022, arXiv preprint arXiv:2206.09328.

[96] M. Janner, J. Fu, M. Zhang, et al., When to trust your model: Modelbasedpolicy optimization, in: Advances in Neural Information ProcessingSystems (NeurIPS), 2018, pp. 12498{12509.

[97] X. Wang, Z. Zhang, W. Zhang, Model-based multi-agent reinforcementlearning: Recent progress and prospects, 2022, arXiv preprintarXiv:2203.10603.

[98] D. Willemsen, M. Coppola, G. C. Croon, MAMBPO: Sample-e_cientmulti-robot reinforcement learning using learned world models, in: 2021IEEE/RSJ International Conference on Intelligent Robots and Systems(IROS), pp. 5635{5640, 2021.

[99] Y. J. Park, Y. S. Cho, S. B. Kim, Multi-agent reinforcement learningwith approximate model learning for competitive games, PloS ONE, vol.14, no. 9, e0222215, 2019.

[100] W. Zhang, X. Wang, J. Shen, et al., Model-based multi-agent policyoptimization with adaptive opponent-wise rollouts, in: Proceedings ofthe International Joint Conference on Arti_cial Intelligence (IJCAI), pp.3384{3391, 2021.

[101] Q. Zhang, C. Lu, A. Garg, et al., Centralized model and explorationpolicy for multi-agent rl, in: Proceedings of the International Conferenceon Autonomous Agents and Multiagent Systems (AAMAS), pp.1500{1508, 2022.

[102] D. Hafner, T. Lillicrap, J. Ba, et al., Dream to control: Learning behaviorsby latent imagination, in: International Conference on LearningRepresentations (ICLR), 2019.

[103] Z. Xu, B. Zhang, Y. Zhan, et al., Mingling foresight with imagination:Model-based cooperative multi-agent reinforcement learning, Advancesin Neural Information Processing Systems (NeurIPS), pp. 11327{11340,2022.

[104] V. Egorov, A. Shpilman, Scalable multi-agent model-based reinforcementlearning, in: Proceedings of the International Conference on AutonomousAgents and Multiagent Systems, pp. 381{390, 2022.

[105] Z. Wang, D. Meger, Leveraging world model disentanglement invalue-based multi-agent reinforcement learning, 2023, arXiv preprintarXiv:2309.04615.

[106] R. Wang, J. C. Kew, D. Lee, et al., Model-based reinforcement learningfor decentralized multiagent rendezvous, in: Conference on RobotLearning, pp. 711{725, 2021.

[107] W. Xiao, Y. Lyu, J. Dolan, Model-based dynamic shielding for safe ande_cient multi-agent reinforcement learning, in: Proceedings of the InternationalConference on Autonomous Agents and Multiagent Systems,pp. 1587{1596, 2023.

[108] A. Mahajan, M. Samvelyan, L. Mao, et al., Tesseract: Tensorised actorsfor multi-agent reinforcement learning, in: Proceedings of the InternationalConference on Machine Learning, pp. 7301{7312, 2021.

[109] Y. Du, C. Ma, Y. Liu, R. Lin, et al., Scalable model-based policyoptimization for decentralized networked systems, in: IEEE/RSJ InternationalConference on Intelligent Robots and Systems, pp. 9019{9026,2022.

[110] X. Yu, J. Jiang, W. Zhang, et al., Model-based opponent modeling, in:Advances in Neural Information Processing Systems, pp. 28208{28221,2022.

[111] S. Han, M. Dastani, S. Wang, Model-based sparse communicationin multi-agent reinforcement learning, in: Proceedings of the InternationalConference on Autonomous Agents and Multiagent Systems, pp.439{447, 2023.

[112] B. P_asztor, A. Krause, I. Bogunovic, E_cient model-based multi-agentmean-_eld reinforcement learning, Transactions on Machine LearningResearch, 2023.

[113] P. G. Sessa, M. Kamgarpour, A. Krause, E_cient model-based multiagentreinforcement learning via optimistic equilibrium computation, in:Proceedings of the International Conference on Machine Learning, pp.19580{19597, 2022.

[114] P. Barde, J. Foerster, D. Nowrouzezahrai, et al., A model-based solutionto the o_ine multi-agent reinforcement learning coordination problem,2023, arXiv preprint arXiv:2305.17198.

[115] D. Han, C. X. Lu, T. Michalak, et al., Multiagent model-based creditassignment for continuous control, in: Proceedings of the InternationalConference on Autonomous Agents and Multiagent Systems, pp.571{579, 2022.

[116] C. Qian, Y. Yu, Z. H. Zhou, Subset selection by pareto optimization,in: Advances in Neural Information Processing Systems, pp. 1774{1782,2015.

[117] Z. Zhu, K. Lin, A. K. Jain, et al., Transfer learning in deep reinforcementlearning: A survey, IEEE Transactions on Pattern Analysis andMachine Intelligence, 2023.

[118] F. L. D. Silva, A. H. R. Costa, A survey on transfer learning for multiagentreinforcement learning systems, Journal of Arti_cial IntelligenceResearch, vol. 64, pp. 645{703, 2019.

[119] S. Omidsha_ei, D. K. Kim, M. Liu, et al., Learning to teach in cooperativemultiagent reinforcement learning, in: Proceedings of the AAAIConference on Arti_cial Intelligence, pp. 6128{6136, 2019.

[120] S. Wadhwania, D. K. Kim, S. Omidsha_ei, et al., Policy distillationand value matching in multiagent reinforcement learning, in:IEEE/RSJ International Conference on Intelligent Robots and Systems,pp. 8193{8200, 2019.

[121] T. Yang, W.Wang, H. Tang, et al., An e_cient transfer learning frameworkfor multiagent reinforcement learning, in: Advances in Neural InformationProcessing Systems, pp. 17037{17048, 2021.

[122] Q. Long, Z. Zhou, A. Gupta, et al., Evolutionary population curriculumfor scaling multi-agent reinforcement learning, in: International Conferenceon Learning Representations, 2019.

[123] W. Wang, T. Yang, Y. Liu, et al., From few to more: Large-scaledynamic multiagent curriculum learning, in: Proceedings of the AAAIConference on Arti_cial Intelligence, pp. 7293{7300, 2020.

[124] S. Hu, F. Zhu, X. Chang, et al., UPDeT: Universal multi-agent reinforcementlearning via policy decoupling with transformers, in: InternationalConference on Learning Representations, 2021.

[125] T. Zhou, F. Zhang, K. Shao, et al., Cooperative multi-agent transferlearning with level-adaptive credit assignment, 2021, arXiv preprintarXiv:2106.00517.

[126] R. Qin, F. Chen, T. Wang, et al., Multi-agent policy transfer via taskrelationship modeling, 2022, arXiv preprint arXiv:2203.04482.

[127] H. Shi, J. Li, J. Mao, et al., Lateral transfer learning for multiagentreinforcement learning, IEEE Transactions on Cybernetics, vol. 53, no.3, pp. 1699{1711, 2023.

[128] A. Mahajan, M. Samvelyan, T. Gupta, et al., Generalization in cooperativemulti-agent systems, 2022, arXiv preprint arXiv:2202.00104.

[129] S. Levine, A. Kumar, G. Tucker, et al., O_ine reinforcement learning:Tutorial, review, and perspectives on open problems, 2020, preprintarXiv:2005.01643.

[130] R. F. Prudencio, M. R. Maximo, E. L. Colombini, A survey on o_inereinforcement learning: Taxonomy, review, and open problems, IEEETransactions on Neural Networks and Learning Systems, 2023.

[131] S. Fujimoto, D. Meger, D. Precup, O_-policy deep reinforcement learningwithout exploration, in: Proceedings of the International Conferenceon Machine Learning, pp. 2052{2062, 2019.

[132] A. Kumar, A. Zhou, G. Tucker, et al., Conservative Q-learning foro_ine reinforcement learning, in: Advances in Neural Information ProcessingSystems, pp. 1179{1191, 2020.

[133] K. Zhang, Z. Yang, H. Liu, et al., Finite-sample analysis for decentralizedbatch multiagent reinforcement learning with networked agents,IEEE Transactions on Automatic Control, vol. 66, no. 12, pp. 5925{5940,2021.

[134] L. Pan, L. Huang, T. Ma, et al., Plan better amid conservatism:O_ine multi-agent reinforcement learning with actor recti_cation, in:Proceedings of the International Conference on Machine Learning, pp.17221{17237, 2022.

[135] W. C. Tseng, T. H. Wang, Y. C. Lin, et al., O_ine multi-agent reinforcementlearning with knowledge distillation, in: Advances in NeuralInformation Processing Systems, pp. 226{237, 2022.

[136] Y. Yang, X. Ma, C. Li, et al., Believe what you see: Implicit constraintapproach for o_ine multi-agent reinforcement learning, in: Advances inNeural Information Processing Systems, pp. 10299{10312, 2021.

[137] J. Jiang, Z. Lu, O_ine decentralized multi-agent reinforcement learning,2021, preprint arXiv:2108.01832.

[138] Q. Tian, K. Kuang, F. Liu, et al., Learning from good trajectoriesin o_ine multi-agent reinforcement learning, 2022, preprintarXiv:2211.15612.

[139] F. Zhang, C. Jia, Y. C. Li, et al., Discovering generalizable multiagentcoordination skills from multi-task o_ine data, in: InternationalConference on Learning Representations, 2023.

[140] C. Formanek, A. Jeewa, J. Shock, et al., O_-the-grid marl: Datasetsand baselines for o_ine multi-agent reinforcement learning, in: Proceedingsof the International Conference on Autonomous Agents andMultiagent Systems, pp. 2442{2444, 2023.

[141] L. Meng, M. Wen, Y. Yang, et al., O_ine pre-trained multi-agent decisiontransformer: One big sequence model conquers all StarCraft IItasks, 2021, preprint arXiv:2112.02845.

[142] L. Meng, J. Ruan, X. Xiong, et al., M3: Modularization for multitaskand multi-agent o_ine pre-training, in: Proceedings of the InternationalConference on Autonomous Agents and Multiagent Systems,pp. 1624{1633, 2023.

[143] X.Wang, X. Zhan, O_ine multi-agent reinforcement learning with coupledvalue factorization, in: Proceedings of the International Conferenceon Autonomous Agents and Multiagent Systems, pp. 2781{2783, 2023.

[144] J. Moos, K. Hansel, H. Abdulsamad, et al., Robust reinforcement learning:A review of foundations and recent advances, Machine Learning andKnowledge Extraction, vol. 4, no. 1, pp. 276{315, 2022.

[145] Y. Song, J. Schneider, Robust reinforcement learning via genetic curriculum,in: Proceedings of the International Conference on Roboticsand Automation, pp. 5560{5566, 2022.

[146] Y. Liang, Y. Sun, R. Zheng, et al., E_cient adversarial training withoutattacking: Worst-case-aware robust reinforcement learning, in: Advancesin Neural Information Processing Systems, pp. 22547{22561,2022.

[147] F. Wu, L. Li, Z. Huang, et al., Crop: Certifying robust policies forreinforcement learning through functional smoothing, in: InternationalConference on Learning Representations, 2022.

[148] J. Guo, Y. Chen, Y. Hao, et al., Towards comprehensive testing onthe robustness of cooperative multi-agent reinforcement learning, in:Proceedings of the 2022 IEEE/CVF Conference on Computer Visionand Pattern Recognition Workshops, pp. 114{121, 2022.

[149] G. Papoudakis, F. Christianos, A. Rahman, et al., Dealing with nonstationarityin multi-agent deep reinforcement learning, 2019, preprintarXiv:1906.04737.

[150] J. Wang, Z. Ren, B. Han, et al., Towards understanding cooperativemulti-agent q-learning with value factorization, in: Advances in NeuralInformation Processing Systems, pp. 29142{29155, 2021.

[151] F. Christianos, G. Papoudakis, M. A. Rahman, et al., Scaling multiagentreinforcement learning with selective parameter sharing, in: Proceedingsof the International Conference on Machine Learning, pp.1989{1998, 2021.

[152] J. Lin, K. Dzeparoska, S. Q. Zhang, et al., On the robustness of cooperativemulti-agent reinforcement learning, in: 2020 IEEE Security andPrivacy Workshops, pp. 62{68, 2020.

[153] T. Heiden, C. Salge, E. Gavves, et al., Robust multi-agent reinforcementlearning with social empowerment for coordination and communication,2020, preprint arXiv:2012.08255

[154] S. Li, Y. Wu, X. Cui, et al., Robust multi-agent reinforcement learningvia minimax deep deterministic policy gradient, in: Proceedings of theAAAI Conference on Arti_cial Intelligence, pp. 4213{4220, 2019.

[155] K. Zhang, T. Sun, Y. Tao, et al., Robust multi-agent reinforcementlearning with model uncertainty, in: Advances in Neural InformationProcessing Systems, pp. 10571{10583, 2020.

[156] Y. Hu, K. Shao, D. Li, et al., Robust multi-agent reinforcement learningdriven by correlated equilibrium, 2021.

[157] W. Xue, W. Qiu, B. An, et al., Mis-spoke or mis-lead: Achieving robustnessin multi-agent communicative reinforcement learning, in: Proceedingsof the International Conference on Autonomous Agents andMultiagent Systems, pp. 1418{1426, 2022.

[158] R. Mitchell, J. Blumenkamp, A. Prorok, Gaussian process based message_ltering for robust multi-agent cooperation in the presence of adversarialcommunication, 2020., preprint arXiv:2012.00508

[159] J. Tu, T. Wang, J. Wang et al., Adversarial attacks on multi-agentcommunication, in: Proc. IEEE/CVF Int. Conf. Comput. Vis., pp.7768{7777, 2021.

[160] T. Phan, T. Gabor, A. Sedlmeier et al., Learning and testing resiliencein cooperative multi-agent systems, in: Proc. Int. Conf. Auton. AgentsMultiAgent Syst., pp. 1055{1063, 2020.

[161] T. Phan, L. Belzner, T. Gabor et al., Resilient multi-agent reinforcementlearning with adversarial value decomposition, in: Proc. AAAIConf. Artif. Intell., pp. 11308{11316, 2021.

[162] L. Yuan, Z. Zhang, K. Xue et al., Robust multi-agent coordinationvia evolutionary generation of auxiliary adversarial attackers, in: Proc.AAAI Conf. Artif. Intell., pp. 11753{11762, 2023.

[163] L. Yuan, T. Jiang, L. Li et al., Robust multi-agent communication viamulti-view message certi_cation, 2023, preprint, arXiv:2305.13936.

[164] L. Yuan, F. Chen, Z. Zhang et al., Communication-robust multi-agentlearning by adaptable auxiliary multi-agent adversary generation, 2023,preprint, arXiv:2305.05116.

[165] C. Wang, Y. Ran, L. Yuan et al., Robust multi-agent reinforcementlearning against adversaries on observation, 2022, vol. 34, no. 39.

[166] S. He, S. Han, S. Su et al., Robust multi-agent reinforcement learningwith state uncertainty, Trans. Mach. Learn. Res., 2023.

[167] M. G. Bellemare, W. Dabney, M. Rowland, Distributional reinforcementlearning, MIT Press, 2023.

[168] W. F. Sun, C. K. Lee, C. Y. Lee, DFAC framework: Factorizingthe value function via quantile mixture for multi-agent distributionalq-learning, in: Proc. Int. Conf. Machine Learn., pp. 9945{9954, 2021.

[169] Q.Wei, X.Wang, R. Yu et al., Rmix: Learning risk-sensitive policies forcooperative reinforcement learning agents, in: Adv. Neural Inf. Process.Syst., pp. 23049{23062, 2021.

[170] J. Oh, J. Kim, M. Jeong, S.-Y. Yun, Toward risk-based optimisticexploration for cooperative multi-agent reinforcement learning, in: Proc.Int. Conf. Auton. Agents Multiagent Syst., pp. 1597{1605, 2023.

[171] J. Hu, Y. Sun, H. Chen et al., Distributional reward estimation fore_ective multi-agent deep reinforcement learning, in: Adv. Neural Inf.Process. Syst., pp. 12619{12632, 2022.

[172] Z. Liu, Y. Fang, Learning adaptable risk-sensitive policies to coordinatein multi-agent general-sum games, 2023, preprint, arXiv:2303.07850.

[173] K. Son, J. Kim, S. Ahn et al., Disentangling sources of risk for distributionalmulti-agent reinforcement learning, in: Proc. Int. Conf. MachineLearn., pp. 20347{20368, 2022.

[174] O. Slumbers, D. H. Mguni, S. B. Blumberg et al., A game-theoreticframework for managing risk in multi-agent systems, in: Proc. Int. Conf.Machine Learn., pp. 32059{32087, 2023.

[175] K. Deb, K. Sindhya, J. Hakanen, Multi-objective optimization, in: DecisionSci., pp. 161{200, CRC Press, 2016.

[176] C. Liu, X. Xu, D. Hu, Multiobjective reinforcement learning: A comprehensiveoverview, IEEE Trans. Syst., Man, Cybern.: Syst., vol. 45,no. 3, pp. 385{398, 2014.

[177] C. F. Hayes, R. Radulescu, E. Bargiacchi et al., A practical guideto multi-objective reinforcement learning and planning, Auton. AgentsMulti-Agent Syst., vol. 36, no. 1, 26, 2022.

[178] R. Radulescu, P. Mannion, D. M. Roijers, A. Now_e, Multi-objectivemulti-agent decision-making: a utility-based analysis and survey, Auton.Agents Multi-Agent Syst., vol. 34, no. 1, 10, 2020.

[179] I. Durugkar, E. Liebman, P. Stone, Balancing individual preferencesand shared objectives in multiagent reinforcement learning, in: Proc.Int. Joint Conf. on Artif. Intell., pp. 2505{2511, 2020.

[180] W. Ropke, Reinforcement learning in multi-objective multi-agentsystems, in: Proc. Int. Conf. Auton. Agents Multiagent Syst., pp.2999{3001, 2023.

[181] R. Radulescu, T. Verstraeten, Y. Zhang et al., Opponent learningawareness and modelling in multi-objective normal form games, NeuralComput. Appl., vol. 34, no. 3, pp. 1759{1781, 2022.

[182] S. Gu, L. Yang, Y. Du et al., A review of safe reinforcement learning:Methods, theory and applications, 2022, preprint, arXiv:2205.10330.

[183] S. Gu, J. Grudzien Kuba, Y. Chen et al., Safe multi-agent reinforcementlearning for multi-robot control, Artif. Intell., vol. 319, 103905,2023.

[184] D. Ding, X. Wei, Z. Yang et al., Provably e_cient generalized lagrangianpolicy optimization for safe multi-agent reinforcement learning,in: Learn. Dyn. Control Conf., pp. 315{332, 2023.

[185] D. Ying, Y. Zhang, Y. Ding et al., Scalable primal-dual actor-criticmethod for safe multi-agent rl with general utilities, 2023, preprint,arXiv:2305.17568.

[186] Z. Zhang, Y. Sun, F. Huang, F. Miao, Safe and robust multi-agentreinforcement learning for connected autonomous vehicles under stateperturbations, 2023, preprint, arXiv:2309.11057.

[187] Z. Wang, Y. Du, A. Sootla et al., CAMA: A new framework for safemulti-agent reinforcement learning using constraint augmentation, 2023.

[188] W. Xiao, Y. Lyu, J. Dolan, Model-based dynamic shielding for safe ande_cient multi-agent reinforcement learning, in: Proc. Int. Conf. Auton.Agents Multiagent Syst., pp. 1587{1596, 2023.

[189] C. Dawson, S. Gao, C. Fan, Safe control with learned certi_cates: Asurvey of neural lyapunov, barrier, and contraction methods for roboticsand control, IEEE Trans. Robot., 2023.

[190] M. Hussonnois, T. G. Karimpanal, S. Rana, Controlled diversity withpreference: Towards learning a diverse set of desired skills, in: Proc. Int.Conf. Auton. Agents Multiagent Syst., pp. 1135{1143, 2023.

[191] L. Yuan, X. Gao, Z. Zheng et al., In situ bidirectional human-robotvalue alignment, Sci. Robot., vol. 7, no. 68, eabm4183, 2022.

[192] N. Xiong, Z. Liu, Z. Wang, Z. Yang, Sample-e_cient multi-agent RL:An optimization perspective, 2023, preprint, arXiv:2310.06243.

[193] M. Zaheer, S. Kottur, S. Ravanbakhsh et al., Deep Sets, in: Proc. 31stInt. Conf. Neural Inf. Process. Syst., pp. 3394{3404, 2017.

[194] M. Huegle, G. Kalweit, B. Mirchevska et al., Dynamic input for deepreinforcement learning in autonomous driving, in: Proc. IEEE/RSJ Int.Conf. Intell. Robots Syst. (IROS), pp. 7566{7573, IEEE, 2019.

[195] G. Shi, W. Honig, S. J. Chung, Neural-swarm: Decentralized closeproximitymultirotor control using learned interactions, in: Proc. IEEEInt. Conf. Robot. Autom. (ICRA), pp. 3241{3247, IEEE, 2020.

[196] G. Shi, W. Honig, X. Shi et al., Neural-swarm2: Planning and controlof heterogeneous multirotor swarms using learned interactions, IEEETrans. Robot., vol. 38, no. 2, 3098436, 2021.

[197] Y. Li, L. Wang, J. Yang et al., Permutation invariant policy optimizationfor mean-_eld multi-agent reinforcement learning: A principled approach,2021, preprint, arXiv:2105.08268.

[198] X. Liu, Y. Tan, Attentive relational state representation in decentralizedmultiagent reinforcement learning, IEEE Trans. Cybern., vol. 52,no. 1, pp. 252{264, 2022.

[199] S. Iqbal, C. D. Witt, B. Peng et al., Randomized entity-wise factorizationfor multi-agent reinforcement learning, in: Proc. Int. Conf. MachineLearn. (ICML), PMLR, pp. 4596{4606, 2021.

[200] S. Batra, Z. Huang, A. Petrenko et al., Decentralized control of quadrotorswarms with end-to-end deep reinforcement learning, in: Conf.Robot Learn., PMLR, pp. 576{586, 2022.

[201] C. Guestrin, D. Koller, R. Parr, Multiagent planning with factoredMDPs, in: Proc. 14th Int. Conf. Neural Inf. Process. Syst.: Natural andSynthetic, pp. 1523{1530, 2001.

[202] R. Nair, P. Varakantham, M. Tambe et al., Networked distributedPOMDPs: A synthesis of distributed constraint optimization andPOMDPs, in: Proc. 20th Nat. Conf. Artif. Intell. Vol. 1, pp. 133{139,2005.

[203] F. A. Oliehoek, S. Whiteson, M. T. J. Spaan, Approximate solutionsfor factored Dec-POMDPs with many agents, in: Proc. Int. Conf. AutonomousAgents Multiagent Syst., pp. 563{570, 2013.

[204] I. J. Liu, R. A. Yeh, A. G. Schwing, PIC: Permutation invariant criticfor multi-agent deep reinforcement learning, in: Conf. Robot Learn.(CoRL), PMLR, pp. 590{602, 2020.

[205] N. Naderializadeh, F. H. Hung, S. Soleyman et al., Graph convolutionalvalue decomposition in multi-agent reinforcement learning, preprint,2020, arXiv:2010.04740.

[206] H. Ryu, H. Shin, J. Park, Multi-agent actor-critic with hierarchicalgraph attention network, in: Proc. AAAI Conf. Artif. Intell. (AAAI),vol. 34, no. 5, pp. 7236{7243, 2020.

[207] Z. Ye, K. Wang, Y. Chen et al., Multi-UAV Navigation for partiallyobservable communication coverage by graph reinforcement learning,IEEE Trans. Mobile Comput., DOI: 10.1109/TMC.2022.3146881, 2021.

[208] W. Bohmer, V. Kurin, S. Whiteson, Deep coordination graphs, in: Int.Conf. Machine Learn. (ICML), PMLR, pp. 980{991, 2020.

[209] Y. Bai, C. Gong, B. Zhang et al., Value function factorisation with hypergraphconvolution for cooperative multi-agent reinforcement learning,2021, preprint, arXiv:2112.06771.

[210] F. A. Oliehoek, M. T. J. Spaan, S. Whiteson et al., Exploiting localityof interaction in factored Dec-POMDPs, in: Proc. 7th Int. Joint Conf.Autonomous Agents Multiagent Syst., pp. 517{524, 2008.

[211] Y. Yang, R. Luo, M. Li et al., Mean _eld multi-agent reinforcementlearning, in: Int. Conf. Machine Learn. (ICML), PMLR, pp. 5571{5580,2018.

[212] M. Lauriere, S. Perrin, S. Girgin et al., Scalable Deep ReinforcementLearning Algorithms for Mean Field Games, in: Proc. The Thirty-ninthInt. Conf. Machine Learn., pp. 12078{12095, 2022.

[213] P. E. Caines, M. Huang, Graphon mean _eld games and the GMFGequations: _-Nash equilibria, in: IEEE 58th Conf. Decision Control(CDC), pp. 286{292, IEEE, 2019.

[214] H. Gu, X. Guo, X. Wei et al., Mean-_eld controls with Q-learningfor cooperative MARL: convergence and complexity analysis, SIAM J.Math. Data Sci., vol. 3, no. 4, pp. 1168{1196, 2021.

[215] J. N. Foerster, Y. M. Assael, N. De Freitas et al., Learning to communicatewith deep multi-agent reinforcement learning, in: Proc. 30th Int.Conf. Neural Inf. Process. Syst., pp. 2145{2153, ACM, New York, 2016.

[216] S. Ukhbaatar, A. Szlam, R. Fergus, Learning multi-agent communicationwith backpropagation, in: Proc. 30th Int. Conf. Neural Inf. Process.Syst., pp. 2252{2260, ACM, New York, 2016.

[217] P. Peng, Y. Wen, Y. Yang et al., Multi-agent bidirectionally coordinatednets: emergence of human-level coordination in learning to playStarCraft combat games, 2017, preprint, arXiv:1703.10069.

[218] C. Guestrin, S. Venkataraman, D. Koller, Context-speci_c multiagentcoordination and planning with factored MDPs, in: Proc. EighteenthNat. Conf. Artif. Intell. Fourteenth Conf. Innovative Appl. Artif. Intell.,pp. 253{259, 2002.

[219] S. Cheng, Coordinating decentralized learning and conict resolutionacross agent boundaries, PhD thesis, The University of North Carolinaat Charlotte, 2012.

[220] W. Boehmer, V. Kurin, S. Whiteson, Deep coordination graphs, in:Proc. Int. Conf. Machine Learn., pp. 980{991, 2020.

[221] J. R. Kok, N. Vlassis, Collaborative multiagent reinforcement learningby payo_ propagation, J. Machine Learn. Res., vol. 7, pp. 1789{1828,2006.

[222] C. Zhang, V. Lesser, Coordinating multi-agent reinforcement learningwith limited communication, in: Proc. Int. Conf. Autonomous AgentsMultiagent Syst., pp. 1101{1108, 2013.

[223] J. Castellini, F. A. Oliehoek, R. Savani, S. Whiteson, The representationalcapacity of action-value networks for multi-agent reinforcementlearning, in: Proc. Int. Conf. Autonomous Agents Multiagent Syst., pp.1862{1864, 2019.

[224] T. Wang, L. Zeng, W. Dong, Q. Yang, Y. Yu, C. Zhang, Context-awaresparse deep coordination graphs, in: Int. Conf. Learn. Representations,2022.

[225] Y. Kang, T.Wang, Q. Yang, X.Wu, C. Zhang, Non-linear coordinationgraphs, in: Adv. Neural Inf. Process. Syst., pp. 25655{25666, 2022.

[226] Q. Yang, W. Dong, Z. Ren, J. Wang, T. Wang, C. Zhang, Selforganizedpolynomial-time coordination graphs, in: Proc. Int. Conf.Machine Learn., pp. 24963{24979, 2022.

[227] J. Jiang, Z. Lu, Learning attentional communication for multi-agentcooperation, in: Adv. Neural Inf. Process. Syst., pp. 7265{7275, 2018.

[228] S. Li, J. K. Gupta, P. Morales, R. Allen, M. J. Kochenderfer, Deepimplicit coordination graphs for multi-agent reinforcement learning, in:Proc. Int. Conf. Autonomous Agents Multiagent Syst., pp. 764{772,2021.

[229] Y. Niu, R. Paleja, M. Gombolay, Multi-agent graph-attention communicationand teaming, in: Proc. Int. Conf. Autonomous Agents MultiagentSyst., pp. 964{973, 2021.

[230] J. Sheng, X. Wang, B. Jin, W. Li, J. Wang, J. Yan, T.-H. Chang, H.Zha, Learning structured communication for multi-agent reinforcementlearning, in: Proc. Int. Conf. Autonomous Agents Multiagent Syst., pp.436{438, 2023.

[231] R. Mirsky, I. Carlucho, A. Rahman, E. Fosong, W. Macke, M. Sridharan,P. Stone, S. V. Albrecht, A survey of Ad hoc teamwork research,in: Eur. Conf. Multi-Agent Syst., pp. 275{293, 2022.

[232] N. Agmon, P. Stone, Leading Ad hoc agents in joint action settings withmultiple teammates, in: Proc. Int. Conf. Autonomous Agents MultiagentSyst., pp. 341{348, 2012.

[233] W. Macke, R. Mirsky, P. Stone, Expected value of communication forplanning in Ad hoc teamwork, in: Proc. AAAI Conf. Artif. Intell., pp.11290{11298, 2021.

[234] S. Barrett, N. Agmon, N. Hazon, S. Kraus, P. Stone, Communicatingwith unknown teammates, in: Proc. Int. Conf. Autonomous AgentsMultiagent Syst., pp. 1433{1434, 2014.

[235] M. Chandrasekaran, A. Eck, P. Doshi, L.-K. Soh, Individual planningin open and typed agent systems, in: Proc. Conf. Uncertainty Artif.Intell., pp. 82{91, 2016.

[236] M. A. Rahman, N. Hopner, F. Christianos, S. V. Albrecht, Towardsopen Ad hoc teamwork using graph-based policy learning, in: Proc. Int.Conf. Machine Learn., pp. 8776{8786, 2021.

[237] P. Gu, M. Zhao, J. Hao, B. An, Online Ad hoc teamwork under partialobservability, in: Int. Conf. Learn. Representations, 2022.

[238] A. Rahman, I. Carlucho, N. Hopner, S. V. Albrecht, A general learningframework for open Ad hoc teamwork using graph-based policy learning,2022, preprint, arXiv:2210.05448.

[239] J. G. Ribeiro, G. Rodrigues, A. Sardinha, F. S. Melo, Teamster: Modelbasedreinforcement learning for Ad hoc teamwork, Arti_cial Intelligence,vol. 104013, 2023.

[240] T. Fujimoto, S. Chatterjee, A. Ganguly, Ad hoc teamwork in: thepresence of adversaries, 2022., preprint, arXiv:2208.05071

[241] E. Fosong, A. Rahman, I. Carlucho, S. V. Albrecht, Few-shot teamwork,2022, preprint, arXiv:2207.09300.

[242] A. Rahman, E. Fosong, I. Carlucho, S. V. Albrecht, Generating teammatesfor training robust Ad hoc teamwork agents via best-responsediversity, Transactions on Machine Learning Research, 2023.

[243] J. Treutlein, M. Dennis, C. Oesterheld, J. Foerster, A new formalism,method and open issues for zero-shot coordination, in: Proc. Int. Conf.Machine Learn., pp. 10413{10423, 2021.

[244] D. Silver, T. Hubert, J. Schrittwieser, I. Antonoglou, M. Lai, A. Guez,M. Lanctot, L. Sifre, D. Kumaran, T. Graepel, et al., A general reinforcementlearning algorithm that masters chess, shogi, and go throughself-play, Science, vol. 362, no. 6419, pp. 1140{1144, 2018.

[245] H. Hu, A. Lerer, A. Peysakhovich, J. Foerster, `Other-play' for zeroshotcoordination, in: Proc. Int. Conf. Machine Learn., pp. 4399{4410,2020.

[246] J. Heinrich, M. Lanctot, D. Silver, Fictitious self-play in extensive-formgames, in: Proc. Int. Conf. Machine Learn., pp. 805{813, 2015.

[247] X. Wang, S. Zhang, W. Zhang, W. Dong, J. Chen, Y. Wen, W. Zhang,Quantifying zero-shot coordination capability with behavior preferringpartners, 2023, preprint, arXiv:2310.05208.

[248] R. Zhao, J. Song, H. Haifeng, Y. Gao, Y. Wu, Z. Sun, W. Yang, Maximumentropy population based training for zero-shot human-AI coordination,in: Proc. AAAI Conf. Artif. Intell., pp. 6145{6153, 2023.

[249] K. Xue, Y. Wang, L. Yuan, C. Guan, C. Qian, Y. Yu, Heterogeneousmulti-agent zero-shot coordination by coevolution, 2022, preprint,arXiv:2208.04957.

[250] D. Muglich, C. Schroeder de Witt, E. van der Pol, S. Whiteson, J. Foerster,Equivariant networks for zero-shot coordination, in: Adv. NeuralInf. Process. Syst., pp. 6410{6423, 2022.

[251] L. Yu, Y. Qiu, Q. Yao, X. Zhang, J. Wang, Improving zero-shotcoordination performance based on policy similarity, 2023, preprint,arXiv:2302.05063.

[252] Y. Li, S. Zhang, J. Sun, Y. Du, Y. Wen, X. Wang, W. Pan, Cooperativeopen-ended learning framework for zero-shot coordination, 2023,preprint, arXiv:2302.04831.

[253] R. Charakorn, P. Manoonpong, N. Dilokthanakul, Generating diversecooperative agents by learning incompatible policies, in: Int. Conf.Learn. Representations, 2023.

[254] C. Yu, J. Gao, W. Liu, B. Xu, H. Tang, J. Yang, Y. Wang, Y. Wu,Learning zero-shot cooperation with humans, assuming humans are biased,in: Int. Conf. Learn. Representations, 2023.

[255] J.-B. Gaya, L. Soulier, L. Denoyer, Learning a subspace of policiesfor online adaptation in reinforcement learning, in: Int. Conf. Learn.Representations, 2022.

[256] E. Fosong, A. Rahman, I. Carlucho, S. V. Albrecht, Few-shot teamwork,2022, preprint, arXiv:2207.09300.

[257] H. Ding, C. Jia, C. Guan, F. Chen, L. Yuan, Z. Zhang, Y. Yu, Coordinationscheme probing for generalizable multi-agent reinforcementlearning, 2023.

[258] H. Nekoei, X. Zhao, J. Rajendran, M. Liu, S. Chandar, Towards fewshotcoordination: Revisiting ad hoc teamplay challenge in the game ofHanabi, 2023, preprint, arXiv:2308.10284.

[259] A. Ajoudani, A. M. Zanchettin, S. Ivaldi, A. Albu-Scha_er, K. Kosuge,O. Khatib, Progress and prospects of the human{robot collaboration,Autonomous Robots, vol. 42, pp. 957{975, 2018.

[260] M. Carroll, R. Shah, M. K. Ho, et al., On the utility of learning abouthumans for human-AI coordination, Adv. Neural Inf. Process. Syst., vol.32, 2019.

[261] N. Van Berkel, M. B. Skov, J. Kjeldskov, human-AI interaction: intermittent,continuous, and proactive, Interactions, vol. 28, no. 6, pp.67{71, 2021.

[262] L. Onnasch, E. Roesler, A taxonomy to structure and analyze human{robot interaction, Int. J. Soc. Robotics, vol. 13, no. 4, pp. 833{849,2021.

[263] X. Puig, T. Shu, S. Li, Z. Wang, Y.-H. Liao, J. B. Tenenbaum, S. Fidler,A. Torralba, Watch-and-help: A challenge for social perception andhuman-AI collaboration, in: Int. Conf. Learn. Representations, 2020.

[264] A. Shih, A. Sawhney, J. Kondic, S. Ermon, D. Sadigh, On the criticalrole of conventions in adaptive human-AI collaboration, in: Int. Conf.Learn. Representations, 2020.

[265] H. Hu, D. J. Wu, A. Lerer, J. Foerster, N. Brown, human-AI coordinationvia human-regularized search and learning, 2022, preprint,arXiv:2210.05125.

[266] J. Hong, A. Dragan, S. Levine, Learning to inuence human behaviorwith o_ine reinforcement learning, preprint, 2023, arXiv:2303.02265.

[267] S. Parekh, D. P. Losey, Learning latent representations to co-adapt tohumans, Autonomous Robots, pp. 1{26, 2023.

[268] M. Li, M. Kwon, D. Sadigh, Inuencing leading and following in human{robot teams, Autonomous Robots, vol. 45, pp. 959{978, 2021.

[269] Y. Li, S. Zhang, J. Sun, W. Zhang, Y. Du, Y. Wen, X. Wang, W. Pan,Tackling cooperative incompatibility for zero-shot human-AI coordination,2023, preprint, arXiv:2306.03034.

[270] A. Kumar AV, S. Rana, A. Shilton, S. Venkatesh, human-AI collaborativeBayesian optimisation, in: Adv. Neural Inf. Process. Syst., pp.16233{16245, 2022.

[271] J. Thumm, F. Trost, M. Altho_, Human-robot gym: Benchmarkingreinforcement learning in human-robot collaboration, 2023, preprint,arXiv:2310.06208.

[272] Z.-H. Zhou, Y. Yu, C. Qian, Evolutionary learning: Advances in theoriesand algorithms, Springer, 2019.

[273] C. Qian, Y. Yu, Z.-H. Zhou, Subset selection by Pareto optimization,in: Adv. Neural Inf. Process. Syst., pp. 1774{1782, 2015.

[274] D. Bloembergen, K. Tuyls, D. Hennes, M. Kaisers, Evolutionary dynamicsof multi-agent learning: A survey, J. Arti_cial Intelligence Research,vol. 53, pp. 659{697, 2015.

[275] S. Omidsha_ei, C. Papadimitriou, G. Piliouras, K. Tuyls, M. Rowland,J.-B. Lespiau, W. M. Czarnecki, M. Lanctot, J. Perolat, R. Munos, _-rank: Multi-agent evaluation by evolution, Scienti_c Reports, vol. 9, no.1, 9937, 2019.

[276] T. Shibata, T. Fukuda, Coordination in evolutionary multi-agentroboticsystem using fuzzy and genetic algorithm, Control EngineeringPractice, vol. 2, no. 1, pp. 103{111, 1994.

[277] G. Dixit, E. Gonzalez, K. Tumer, Diversifying behaviors for learningin asymmetric multiagent systems, in: Proc. Genetic and EvolutionaryComputation Conference, pp. 350{358, 2022.

[278] J. Shao, Y. Qu, C. Chen, H. Zhang, X. Ji, Counterfactual conservativeQ learning for o_ine multi-agent reinforcement learning, 2023, preprint,arXiv:2309.12696.

[279] S. Majumdar, S. Khadka, S. Miret, S. McAleer, K. Tumer, Evolutionaryreinforcement learning for sample-e_cient multiagent coordination,in: Proc. Int. Conf. Machine Learn., pp. 6651{6660, 2020.

[280] G. Dixit, K. Tumer, Balancing teams with quality-diversity for heterogeneousmultiagent coordination, in: Proc. Genetic and EvolutionaryComputation Conference Companion, pp. 236{239, 2022.

[281] P. Li, J. Hao, H. Tang, Y. Zheng, X. Fu, RACE: Improve multi-agentreinforcement learning with representation asymmetry and collaborativeevolution, in: Proc. Int. Conf. Machine Learn., pp. 19490{19503, 2023.

[282] G. Dixit, K. Tumer, Learning synergies for multi-objective optimizationin asymmetric multiagent systems, in: Proc. Genetic and EvolutionaryComputation Conference, pp. 447{455, 2023.

[283] Q. Long, Z. Zhou, A. Gupta, et al., Evolutionary population curriculumfor scaling multi-agent reinforcement learning, in: Int. Conf. Learn.Representations, 2019.

[284] L. Yuan, L. Li, Z. Zhang, et al., Learning to coordinate with anyone,2023, preprint, arXiv:2309.12633.

[285] L. Yuan, Z. Zhang, K. Xue, et al., Robust multi-agent coordinationvia evolutionary generation of auxiliary adversarial attackers, in: Proc.AAAI Conf. Artif. Intell., pp. 11753{11762, 2023.

[286] L. Yuan, F. Chen, Z. Zhang, et al., Communication-robust multi-agentlearning by adaptable auxiliary multi-agent adversary generation, 2023,preprint, arXiv:2305.05116.

[287] Z. Liu, Y. Zhu, C. Chen, NA2Q: Neural attention additive model forinterpretable multi-agent Q-learning, 2023, preprint, arXiv:2304.13383.

[288] M. Wen, J. Grudzien Kuba, R. Lin, W. Zhang, Y. Wen, J. Wang, Y.Yang, Multi-agent reinforcement learning is a sequence modeling problem,in: Adv. Neural Inf. Process. Syst., pp. 16509{16521, 2022.

[289] J. Hu, Multi-agent coordination: Theory and applications, Universityof California, Berkeley, 2003.

[290] J. Yang, A. Nakhaei, D. Isele, K. Fujimura, H. Zha, CM3: Cooperativemulti-goal multi-stage multi-agent reinforcement learning, in: Int. Conf.Learn. Representations, 2019.

[291] J. Z. Leibo, V. Zambaldi, M. Lanctot, J. Marecki, T. Graepel, Multiagentreinforcement learning in sequential social dilemmas, in: Proc.Conf. Autonomous Agents MultiAgent Syst., pp. 464{473, 2017.

[292] H. Zhang, G. Li, C. H. Liu, G. Wang, J. Tang, HIMACMIC: Hierarchicalmulti-agent deep reinforcement learning with dynamic asynchronousmacro strategy, in: Proc. 29th ACM SIGKDD Conf. Knowledge DiscoveryData Mining, pp. 3239{3248, 2023.

[293] Q. Fu, T. Qiu, J. Yi, Z. Pu, S. Wu, Concentration network for reinforcementlearning of large-scale multi-agent systems, in: Proc. AAAIConf. Artif. Intell., pp. 9341{9349, 2022.

[294] W. Qiu, W. Wang, R. Wang, B. An, Y. Hu, S. Obraztsova, Z. Rabinovich,J. Hao, Y. Chen, C. Fan, O_-beat multi-agent reinforcementlearning, 2022, preprint, arXiv:2205.1371.

[295] H. Fu, H. Tang, J. Hao, Z. Lei, Y. Chen, C. Fan, Deep multi-agentreinforcement learning with discrete-continuous hybrid action spaces,in: Proc. Int. Joint Conf. Arti_cial Intelligence, pp. 2329{2335, 2019.

[296] S. J. Grimbly, J. Shock, A. Pretorius, Causal multi-agent reinforcementlearning: Review and open problems, 2021, preprint, arXiv:2111.06721.

[297] J. Chen, Y. Zhang, Y. Xu, H. Ma, H. Yang, J. Song, Y. Wang, Y.Wu, Variational automatic curriculum learning for sparse-reward cooperativemulti-agent problems, in: Adv. Neural Inf. Process. Syst., pp.9681{9693, 2021.

[298] F. L. Da Silva, G.Warnell, A. H. Reali Costa, P. Stone, Agents teachingagents: A survey on inter-agent transfer learning, Autonomous AgentsMulti-Agent Syst., vol. 34, pp. 1{17, 2020.

[299] N. A. Grupen, B. Selman, D. D. Lee, Cooperative multi-agent fairnessand equivariant policies, in: Proc. AAAI Conf. Artif. Intell., pp.9350{9359, 2022.

[300] F. L. Da Silva, R. Glatt, A. H. Reali Costa, MOO-MDP: An objectorientedrepresentation for cooperative multiagent reinforcement learning,IEEE Trans. Cybernetics, vol. 49, no. 2, pp. 567{579, 2017.

[301] J. M. Hendrickx, S. Martin, Open multi-agent systems: Gossiping withrandom arrivals and departures, in: Proc. IEEE Annual Conf. Decisionand Control, pp. 763{768, 2017.

[302] J. Cohen, J. S. Dibangoye, A.-I. Mouaddib, Open DecentralizedPOMDPs, in: Proc. IEEE Int. Conf. Tools with Arti_cial Intelligence,pp. 977{984, 2017.

[303] J. Cohen, A.-I. Mouaddib, Power indices for team reformation planningunder uncertainty, in: Proc. Int. Conf. Autonomous Agents MultiAgentSyst., pp. 1886{1888, 2019.

[304] A. Eck, M. Shah, P. Doshi, L.-K. Soh, Scalable decision-theoretic planningin open and typed multiagent systems, in: Proc. AAAI Conf. Artif.Intell., pp. 7127{7134, 2020.

[305] A. Kakarlapudi, G. Anil, A. Eck, P. Doshi, L.-K. Soh, Decisiontheoreticplanning with communication in open multiagent systems, in:Proc. Conf. Uncertainty Artif. Intell., pp. 938{948, 2022.

[306] M. A. Rahman, N. Hopner, F. Christianos, S. V. Albrecht, Towardsopen ad hoc teamwork using graph-based policy learning, in: Proc. Int.Conf. Machine Learn., pp. 8776{8786, 2021.

[307] A. Eck, L.-K. Soh, P. Doshi, Decision-making in open agent systems,AI Magazine, 2023.

[308] G. Conole, Designing for Learning in An Open World, vol. 4, SpringerScience & Business Media, 2012.

[309] Z.-H. Zhou, Open-environment machine learning, National Science Review,vol. 9, no. 8, 2022.

[310] A. Song, A little taxonomy of open-endedness, in: ICLR Workshop onAgent Learning in Open-Endedness, 2022.

[311] C. Geng, S.-j. Huang, S. Chen, Recent advances in open set recognition:A survey, IEEE Trans. Pattern Anal. Mach. Intell., vol. 43, no. 10, pp.3614{3631, 2020.

[312] J. Parmar, S. Chouhan, V. Raychoudhury, S. Rathore, Open-worldmachine learning: Applications, challenges, and opportunities, ACMComput. Surv., vol. 55, no. 10, pp. 1{37, 2023.

[313] B.-J. Hou, L. Zhang, Z.-H. Zhou, Learning with feature evolvablestreams, IEEE Trans. Knowl. Data Eng., vol. 33, no. 6, pp. 2602{2615,2019.

[314] G. Kim, C. Xiao, T. Konishi, Z. Ke, B. Liu, Open-world continuallearning: Unifying novelty detection and continual learning, 2023,preprint, arXiv:2304.10038.

[315] D. Grbic, R. B. Palm, E. Najarro, C. Glanois, S. Risi, Evocraft: A newchallenge for open-endedness, in: Appl. Evol. Comput., Springer, pp.325{340, 2021.

[316] Z.-H. Zhou, Open-environment machine learning, National Science Review,vol. 9, no. 8, 2022.

[317] M. Xu, Z. Liu, P. Huang, W. Ding, Z. Cen, B. Li, D. Zhao, Trustworthyreinforcement learning against intrinsic vulnerabilities: Robustness,safety, and generalizability, 2022, preprint, arXiv:2209.08025.

[318] R. Wang, J. Lehman, A. Rawal, J. Zhi, Y. Li, J. Clune, K. Stanley, EnhancedPOET: Open-ended reinforcement learning through unboundedinvention of learning challenges and their solutions, in: Proc. Int. Conf.Mach. Learn., pp. 9940{9951, 2020.

[319] H. Yuan, C. Zhang, H.Wang, F. Xie, P. Cai, H. Dong, Z. Lu, Plan4MC:Skill reinforcement learning and planning for open-world Minecrafttasks, 2023, preprint, arXiv:2303.16563.

[320] R. Meier, A. Mujika, Open-ended reinforcement learning with neuralreward functions, in: Adv. Neural Inf. Process. Syst., pp. 2465{2479,2022.

[321] M. Matthews, M. Samvelyan, J. Parker-Holder, E. Grefenstette, T.Rocktaschel, SkillHack: A benchmark for skill transfer in open-endedreinforcement learning, in: ICLR Workshop on Agent Learn in Open-Endedness, 2022.

[322] Open Ended Learning Team, A. Stooke, A. Mahajan, C. Barros, C.Deck, J. Bauer, J. Sygnowski, M. Trebacz, M. Jaderberg, M. Mathieu,et al., Open-ended learning leads to generally capable agents, 2021,preprint, arXiv:2107.12808.

[323] Adaptive Agent Team, J. Bauer, K. Baumli, S. Baveja, F. Behbahani,A. Bhoopchand, N. Bradley-Schmieg, M. Chang, N. Clay, A. Collister,et al., Human-timescale adaptation in an open-ended task space, 2023,preprint, arXiv:2301.07608.

[324] R. Kirk, A. Zhang, E. Grefenstette, T. Rocktaschel, A survey of zeroshotgeneralisation in deep reinforcement learning, J. Artif. Intell. Res.,vol. 76, pp. 201{264, 2023.

[325] D. Balduzzi, M. Garnelo, Y. Bachrach, W. Czarnecki, J. Perolat, M.Jaderberg, T. Graepel, Open-ended learning in symmetric zero-sumgames, in: Proc. Int. Conf. Mach. Learn., pp. 434{443, 2019.

[326] P. Muller, S. Omidsha_ei, M. Rowland, et al., A Generalized TrainingApproach for Multiagent Learning, in: The 8th Int. Conf. Learn.Representations, ICLR, 2020, pp. 1-35.

[327] S. McAleer, J. Lanier, R. Fox, et al., Pipeline PSRO: A scalable approachfor _nding approximate Nash equilibria in large games, in: Adv.Neural Inf. Process. Syst., vol. 33, pp. 20238-20248, 2020.

[328] S. Liu, M. Lanctot, L. Marris, et al., Simplex neural population learning:Any-mixture Bayes-optimality in symmetric zero-sum games, 2022,preprint, arXiv:2205.15879.

[329] X. Feng, O. Slumbers, Y. Yang, et al., Neural auto-curricula in twoplayerzero-sum games, in: Proc. Thirty-_fth Conf. Neural Inf. Process.Syst., 2021, pp. 3504-3517.

[330] S. Li, X. Wang, J. Cerny, et al., O_ine equilibrium _nding, 2022,preprint, arXiv:2207.05285.

[331] L. C. Dinh, Y. Yang, Z. Tian, et al., Online double oracle, 2021,preprint, arXiv:2103.07780.

[332] S. McAleer, K. Wang, M. Lanctot, et al., Anytime optimal PSRO fortwo-player zero-sum games, 2022, preprint, arXiv:2201.07700.

[333] S. McAleer, J. B. Lanier, K. Wang, et al., Self-play PSRO: Towardoptimal populations in two-player zero-sum games, 2022, preprint,arXiv:2207.06541.

[334] Z. Li, M. Lanctot, K. R. McKee, et al., Combining Tree-Search, GenerativeModels, and Nash Bargaining Concepts in Game-Theoretic ReinforcementLearning, 2023, preprint, arXiv:2302.00797.

[335] Y. Wang, M. P. Wellman, Regularization for Strategy Exploration inEmpirical Game-Theoretic Analysis, 2023, preprint, arXiv:2302.04928.

[336] M. O. Smith, M. P.Wellman, Co-Learning Empirical Games andWorldModels, 2023, preprint, arXiv:2305.14223.

[337] Y. Hu, H. Li, C. Han, et al., SC-PSRO: A Uni_ed Strategy LearningMethod for Normal-form Games, 2023, preprint, arXiv:2308.12520.

[338] L. Marris, I. Gemp, T. Anthony, et al., Turbocharging solution concepts:Solving NEs, CEs and CCEs with neural equilibrium solvers, in:Adv. Neural Inf. Process. Syst., vol. 35, pp. 5586-5600, 2022.

[339] J. Yao, W. Liu, H. Fu, et al., Policy Space Diversity for Non-TransitiveGames, 2023, preprint, arXiv:2306.16884.

[340] O. Slumbers, D. H. Mguni, S. B. Blumberg, et al., A Game-TheoreticFramework for Managing Risk in Multi-Agent Systems, in: Int. Conf.Mach. Learn., PMLR, 2023, pp. 32059-32087.

[341] S. McAleer, J. B. Lanier, K. A. Wang, et al., XDO: A double oraclealgorithm for extensive-form games, in: Adv. Neural Inf. Process. Syst.,vol. 34, pp. 23128-23139, 2021.

[342] X. Tang, S. M. McAleer, Y. Yang, Regret-Minimizing Double Oraclefor Extensive-Form Games, in: Int. Conf. Mach. Learn., PMLR, 2023,pp. 33599-33615.

[343] C. Konicki, M. Chakraborty, M. P. Wellman, Exploiting Extensive-Form Structure in Empirical Game-Theoretic Analysis, in: Int. Conf.Web Internet Econ., Springer, Cham, 2022, pp. 132-149.

[344] S. M. McAleer, G. Farina, G. Zhou, et al., Team-PSRO for LearningApproximate TMECor in Large Team Games via Cooperative ReinforcementLearning, in: Thirty-seventh Conf. Neural Inf. Process. Syst.,2023.

[345] Z. Xu, Y. Liang, C. Yu, et al., Fictitious Cross-Play: Learning GlobalNash Equilibrium in Mixed Cooperative-Competitive Games, in: Proc.2023 Int. Conf. Autonomous Agents Multiagent Syst., 2023, pp. 1053-1061.

[346] C. Sun, Y. Zhang, Y. Zhang, et al., Mastering Asymmetrical MultiplayerGame with Multi-Agent Asymmetric-Evolution ReinforcementLearning, 2023, preprint, arXiv:2304.10124.

[347] Y. Li, S. Zhang, J. Sun, et al., Cooperative Open-endedLearning Framework for Zero-shot Coordination, 2023, preprint,arXiv:2302.04831.

[348] W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min, B.Zhang, J. Zhang, Z. Dong, et al., A survey of large language models,2023, preprint, arXiv:2303.18223.

[349] W. Ye, Y. Zhang, M. Wang, S. Wang, X. Gu, P. Abbeel, Y. Gao,Foundation reinforcement learning: Towards embodied generalist agentswith foundation prior assistance, 2023, preprint, arXiv:2310.02635.

[350] S. Yang, O. Nachum, Y. Du, J. Wei, P. Abbeel, D. Schuurmans, Foundationmodels for decision-making: Problems, methods, and opportunities,2023, preprint, arXiv:2303.04129.

[351] S. Reed, K. Zolna, E. Parisotto, S. G_omez Colmenarejo, A. Novikov,G. Barth-maron, M. Gim_enez, Y. Sulsky, J. Kay, J. T. Springenberg, etal., A generalist agent, Trans. Mach. Learn. Res., 2022.

[352] D. Hafner, J. Pasukonis, J. Ba, T. Lillicrap, Mastering diverse domainsthrough world models, 2023, preprint, arXiv:2301.04104.

[353] L. Chen, K. Lu, A. Rajeswaran, K. Lee, A. Grover, M. Laskin, P.Abbeel, A. Srinivas, I. Mordatch, Decision transformer: Reinforcementlearning via sequence modeling, in: Adv. Neural Inf. Process. Syst., pp.15084{15097, 2021.

[354] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.Gomez, L. Kaiser, I. Polosukhin, Attention is all you need, in: Adv.Neural Inf. Process. Syst., pp. 6000{6010, 2017.

[355] S. Hu, L. Shen, Y. Zhang, Y. Chen, D. Tao, On transforming reinforcementlearning by transformer: The development trajectory, 2022,preprint, arXiv:2212.14164.

[356] M. Wen, J. G. Kuba, R. Lin, W. Zhang, Y. Wen, J. Wang, Y. Yang,Multi-agent reinforcement learning is a sequence modeling problem, in:Adv. Neural Inf. Process. Syst., pp. 16509{16521, 2022.

[357] R. Wang, W. Wang, X. Zeng, L. Wang, Z. Lian, Y. Gao, F. Liu, S. Li,X. Wang, Q. Fu, Y. Wei, L. Huang, L. Zheng, Z. Rabinovich, B. An,Multi-agent multi-game entity transformer, 2023.

[358] Z. Zhu, M. Liu, L. Mao, B. Kang, M. Xu, Y. Yu, S. Ermon, W.Zhang, Madi_: O_ine multi-agent learning with di_usion models, 2023,preprint, arXiv:2305.17330.

[359] Z. Li, L. Pan, L. Huang, Beyond conservatism: Di_usion policiesin o_ine multi-agent reinforcement learning, 2023, preprint,arXiv:2307.01472.

[360] T. Li, J. Guevara, X. Xie, Q. Zhu, Self-con_rming transformer for locallyconsistent online adaptation in multi-agent reinforcement learning,2023, preprint, arXiv:2310.04579.

[361] J. S. Park, J. C. O'Brien, C. J. Cai, M. R. Morris, P. Liang, M. S.Bernstein, Generative agents: Interactive simulacra of human behavior,2023, preprint, arXiv:2304.03442.

[362] A. W. Hanjie, V. Y. Zhong, K. Narasimhan, Grounding language toentities and dynamics for generalization in reinforcement learning, in:Int. Conf. Mach. Learn., PMLR, pp. 4051-4062, 2021.

[363] J. Jeon, W. Kim, W. Jung, et al., Maser: Multi-agent reinforcementlearning with subgoals generated from experience replay bu_er, in: Int.Conf. Mach. Learn., PMLR, pp. 10041-10052, 2022.

[364] Z. Ding, W. Zhang, J. Yue, et al., Entity divider with language groundingin multi-agent reinforcement learning, in: Int. Conf. Mach. Learn.,PMLR, pp. 8103-8119, 2023.

[365] W. Li, D. Qiao, B. Wang, et al., Semantically Aligned Task Decompositionin Multi-Agent Reinforcement Learning, 2023, preprint,arXiv:2305.10865.

[366] C. Zhang, K. Yang, S. Hu, et al., Proagent: Building proactive cooperativeAI with large language models, 2023, preprint, arXiv:2308.11339.

[367] Y. Wu, X. Tang, T. M. Mitchell, et al., SmartPlay: A Benchmark forLLMs as Intelligent Agents, 2023, preprint, arXiv:2310.01557.

[368] L. Xu, Z. Hu, D. Zhou, et al., MAgIC: Investigation of Large LanguageModel Powered Multi-Agent in Cognition, Adaptability, Rationality andCollaboration, preprint, arXiv:2311.08562.

[369] W. Hua, L. Fan, L. Li, et al., War and Peace (WarAgent): LargeLanguage Model-based Multi-Agent Simulation of World Wars, 2023,preprint, arXiv:2311.17227.

[370] Y. Sun, C. Yu, J. Zhao, et al., Self Generated Wargame AI: DoubleLayer Agent Task Planning Based on Large Language Model, 2023,preprint, arXiv:2312.01090.

[371] W. Ma, Q. Mi, X. Yan, et al., Large Language Models Play StarCraft II:Benchmarks and A Chain of Summarization Approach, 2023, preprint,arXiv:2312.11865.

[372] G. Li, H. A. A. K. Hammoud, H. Itani, et al., Camel: Communicativeagents for 'mind' exploration of large scale language model society, 2023,preprint, arXiv:2303.17760.

[373] C. Qian, X. Cong, C. Yang, et al., Communicative agents for softwaredevelopment, 2023, preprint, arXiv:2307.07924.

[374] J. S. Park, J. C. O'Brien, C. J. Cai, et al., Generative agents: Interactivesimulacra of human behavior, 2023, preprint, arXiv:2304.03442.

[375] S. Hong, X. Zheng, J. Chen, et al., Metagpt: Meta programming formulti-agent collaborative framework, 2023, preprint, arXiv:2308.00352.

[376] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, O. Klimov, ProximalPolicy Optimization Algorithms, 2017.

[377] P. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, D. Amodei,Deep Reinforcement Learning from Human Preferences, 2017.

[378] J. Kreutzer, S. Khadivi, E. Matusov, S. Riezler, Can Neural MachineTranslation Be Improved with User Feedback in Proceedings of NAACLHLT,pp. 92{105, 2018.

[379] D. M. Ziegler, N. Stiennon, J. Wu, T. B. Brown, A. Radford, D.Amodei, P. Christiano, G. Irving, Fine-Tuning Language Models fromHuman Preferences, 2019, arXiv preprint arXiv:1909.08593.

[380] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin,C. Zhang, S. Agarwal, K. Slama, A. Ray, et al., Training LanguageModels to Follow Instructions with Human Feedback, in: Advances inNeural Information Processing Systems, vol. 35, pp. 27730{27744, 2022.

[381] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei,N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, et al., LLaMa 2:Open Foundation and Fine-Tuned Chat Models, 2023, arXiv preprintarXiv:2307.09288.

[382] Z. Yuan, H. Yuan, C. Tan, W.Wang, S. Huang, F. Huang, RRHF: RankResponses to Align Language Models with Human Feedback withoutTears, 2023, arXiv preprint arXiv:2304.05302.

[383] R. Rafailov, A. Sharma, E. Mitchell, S. Ermon, C. D. Manning, C.Finn, Direct Preference Optimization: Your Language Model is Secretlya Reward Model, 2023, arXiv preprint arXiv:2305.18290.

[384] R. A. Bradley, M. E. Terry, Rank Analysis of Incomplete Block Designs:I. The Method of Paired Comparisons, Biometrika, vol. 39, no. 3/4, pp.324{345, doi: https://doi.org/10.2307/2334029, 1952.

[385] R. J. Williams, Simple Statistical Gradient-Following Algorithmsfor Connectionist Reinforcement Learning, Mach. Learn., vol. 8, no.3{4, pp. 229{256, ISSN 0885-6125. doi: 10.1007/BF00992696. URLhttps://doi.org/10.1007/BF00992696, 1992.

[386] Y. Bai, S. Kadavath, S. Kundu, A. Askell, J. Kernion, A. Jones, A.Chen, A. Goldie, A. Mirhosseini, C. McKinnon, et al., ConstitutionalAI: Harmlessness from AI Feedback, 2022.

[387] P. Cheng, Y. Yang, J. Li, Y. Dai, N. Du, Adversarial Preference Optimization,2023, arXiv preprint arXiv:2311.08045.

[388] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,S. Ozair, A. Courville, Y. Bengio, Generative Adversarial Nets, in: Advancesin Neural Information Processing Systems, vol. 27, 2014.

[389] M. Arjovsky, S. Chintala, L. Bottou, Wasserstein Generative AdversarialNetworks, in: Proceedings of the International Conference on MachineLearning, pp. 214{223, PMLR, 2017.

[390] P. Abbeel, A. Y. Ng, Apprenticeship Learning via Inverse ReinforcementLearning, in: Proceedings of the Twenty-First International Conferenceon Machine Learning, 2004.

